{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchvision.models.detection.transform import resize_boxes\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n",
    "from torchvision.ops import MultiScaleRoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_labels=6):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.annotations = []\n",
    "        self.max_labels = max_labels\n",
    "\n",
    "        for root, _, files in os.walk(os.path.join(data_dir, \"images\")):\n",
    "            for file in files:\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    label_path = os.path.join(data_dir, \"labels\", file.replace(\".jpg\", \".txt\"))\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.annotations.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        annotation_path = self.annotations[idx]\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(annotation_path, 'r') as label_file:\n",
    "            lines = label_file.read().splitlines()\n",
    "            labels = []\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 6:\n",
    "                    label = [int(parts[0])] + [float(p) for p in parts[1:]]\n",
    "                    labels.append(label)\n",
    "\n",
    "            while len(labels) < self.max_labels:\n",
    "                labels.append([0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "            labels = labels[:self.max_labels]\n",
    "\n",
    "        # Reformat labels to match the expected shape [N, 4]\n",
    "        num_labels = len(labels)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).view(num_labels, -1)\n",
    "\n",
    "        return F.to_tensor(image), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directories\n",
    "train_data_dir = 'C:/Users/User/Desktop/UoM 3rd Year Project/Code/Football-Player-Detection-Demo-1/train'\n",
    "valid_data_dir = 'C:/Users/User/Desktop/UoM 3rd Year Project/Code/Football-Player-Detection-Demo-1/valid'\n",
    "\n",
    "# Define data loaders for training and validation\n",
    "train_dataset = CustomObjectDetectionDataset(train_data_dir)\n",
    "valid_dataset = CustomObjectDetectionDataset(valid_data_dir)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "model = create_model(num_classes=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([2, 4, 6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\UoM 3rd Year Project\\Code\\test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/test.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m labels \u001b[39m=\u001b[39m labels[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlong()  \u001b[39m# Extract the class labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/test.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m targets \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m: boxes, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: labels}]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/test.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/test.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m'\u001b[39m\u001b[39mloss_classifier\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Choose the appropriate loss as per your use case.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/test.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:67\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     65\u001b[0m boxes \u001b[39m=\u001b[39m target[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(boxes, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 67\u001b[0m     torch\u001b[39m.\u001b[39;49m_assert(\n\u001b[0;32m     68\u001b[0m         \u001b[39mlen\u001b[39;49m(boxes\u001b[39m.\u001b[39;49mshape) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39mand\u001b[39;49;00m boxes\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[0;32m     69\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mExpected target boxes to be a tensor of shape [N, 4], got \u001b[39;49m\u001b[39m{\u001b[39;49;00mboxes\u001b[39m.\u001b[39;49mshape\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     torch\u001b[39m.\u001b[39m_assert(\u001b[39mFalse\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected target boxes to be of type Tensor, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(boxes)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:1404\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[0;32m   1403\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[1;32m-> 1404\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([2, 4, 6])."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reformat labels to match the expected shape [N, 4]\n",
    "        num_labels = labels.size(0)\n",
    "        boxes = labels[:, 1:5]  # Extract the bounding box coordinates\n",
    "        labels = labels[:, 0].long()  # Extract the class labels\n",
    "\n",
    "        targets = [{'boxes': boxes, 'labels': labels}]\n",
    "\n",
    "        outputs = model(images, targets)\n",
    "        loss = outputs['loss_classifier']  # Choose the appropriate loss as per your use case.\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Training Epoch [{epoch + 1}/{num_epochs}]: Loss: {average_loss:.4f}')\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(valid_loader)\n",
    "print(f'Validation Epoch [{epoch + 1}/{num_epochs}]: Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
