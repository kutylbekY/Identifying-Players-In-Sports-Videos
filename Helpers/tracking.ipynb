{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PlayerTrackingModel(nn.Module):\n",
    "#     def __init__(self, input_channels, num_classes):\n",
    "#         super(PlayerTrackingModel, self).__init__()\n",
    "#         # Define the layers for your player tracking model here\n",
    "#         self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  # Add max pooling layer to reduce spatial dimensions\n",
    "#         self.fc1 = nn.Linear(64 * 160 * 160, 128)  # Adjust the input size based on 640x640 images\n",
    "#         self.fc2 = nn.Linear(128, num_classes)  # Modify num_classes to 6\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass for your model\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)  # Apply max pooling to reduce spatial dimensions\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool(x)  # Apply max pooling again\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "# Create a Faster R-CNN model\n",
    "def create_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_labels=6):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.max_labels = max_labels\n",
    "\n",
    "        for root, _, files in os.walk(os.path.join(data_dir, \"images\")):\n",
    "            for file in files:\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    label_path = os.path.join(data_dir, \"labels\", file.replace(\".jpg\", \".txt\"))\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.labels[idx]\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = transforms.ToTensor()(image)\n",
    "\n",
    "        with open(label_path, 'r') as label_file:\n",
    "            lines = label_file.read().splitlines()\n",
    "            labels = []\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 6:  # Ensure there are 6 values in each line\n",
    "                    label = [int(parts[0])] + [float(p) for p in parts[1:]]\n",
    "                    labels.append(label)\n",
    "            \n",
    "            # Pad labels to have the same shape\n",
    "            while len(labels) < self.max_labels:\n",
    "                labels.append([0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            \n",
    "            labels = labels[:self.max_labels]  # Limit to max_labels labels\n",
    "\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PlayerTrackingModel\n",
    "model = create_model(num_classes=5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_data_dir = 'C:/Users/User/Desktop/UoM 3rd Year Project/Code/Football-Player-Detection-Demo-1/train'\n",
    "valid_data_dir = 'C:/Users/User/Desktop/UoM 3rd Year Project/Code/Football-Player-Detection-Demo-1/valid'\n",
    "test_data_dir = 'C:/Users/User/Desktop/UoM 3rd Year Project/Code/Football-Player-Detection-Demo-1/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders for training, validation, and test sets\n",
    "train_dataset = CustomObjectDetectionDataset(train_data_dir)\n",
    "valid_dataset = CustomObjectDetectionDataset(valid_data_dir)\n",
    "test_dataset = CustomObjectDetectionDataset(test_data_dir)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\UoM 3rd Year Project\\Code\\tracking.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/tracking.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/tracking.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/tracking.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images, labels)  \u001b[39m# Provide labels to the model during training\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/tracking.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/UoM%203rd%20Year%20Project/Code/tracking.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:65\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m targets:\n\u001b[1;32m---> 65\u001b[0m         boxes \u001b[39m=\u001b[39m target[\u001b[39m\"\u001b[39;49m\u001b[39mboxes\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     66\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(boxes, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     67\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[0;32m     68\u001b[0m                 \u001b[39mlen\u001b[39m(boxes\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m boxes\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[0;32m     69\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected target boxes to be a tensor of shape [N, 4], got \u001b[39m\u001b[39m{\u001b[39;00mboxes\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     70\u001b[0m             )\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available, otherwise use CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move your model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the labels are also moved to the same device as the model (e.g., GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, labels)  # Provide labels to the model during training\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Training Epoch [{epoch+1}/{num_epochs}]: Loss: {average_loss:.4f}')\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(valid_loader)\n",
    "print(f'Validation Epoch [{epoch+1}/{num_epochs}]: Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "average_loss = total_loss / len(test_loader)\n",
    "accuracy = (total_correct / total_samples) * 100\n",
    "print(f'Testing: Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'player_tracking_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
